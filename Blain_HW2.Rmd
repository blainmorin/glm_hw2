---
title: "Homework #2"
author: "Blain Morin"
date: "March 1, 2018"
output: 
  html_document:
    df_print: paged
    theme: journal
---
```{r, echo=FALSE, results='hide', include=FALSE}

library(tidyverse)
library(gridExtra)
library(readr)
library(sjPlot)
library(knitr)
library(car)
Wine = read_csv("Wine.csv")
attach(Wine)
```

#Question 1: Wine

####a. Plot mortality vs. wine; log mortality vs. wine, mortality vs. log wine and log mortality vs. log wine. Which one looks more linear?

```{r, echo=FALSE}

nolog = ggplot(data = Wine) + geom_point(aes(x = WINE, y = MORTALITY), color = "red")  

logwine = ggplot(data = Wine) + geom_point(aes(x = log(WINE), y = MORTALITY), color = "purple") 

logmortality = ggplot(data = Wine) + geom_point(aes(x = WINE, y = log(MORTALITY)), color = "blue") 

logboth = ggplot(data = Wine) + geom_point(aes(x = log(WINE), y = log(MORTALITY)), color = "green") 

grid.arrange(nolog, logwine, logmortality, logboth, nrow = 2, top = "Figure 1")


```

In figure 1, the green plot looks the most linear. This is the plot where both wine consumption and heart disease mortality rates are log transformed. 


####b. Fit four linear regression models corresponding to the four plots and report the regression equation and R-squared. Which model do you think is best?

```{r, echo = FALSE, results = "hide", message = FALSE}

lm_nolog = lm(MORTALITY~WINE)
lm_logwine =lm(MORTALITY~log(WINE))
lm_logmortality = lm(log(MORTALITY)~WINE)
lm_logboth = lm(log(MORTALITY)~log(WINE))

lmlist = list(lm_nolog, lm_logwine, lm_logmortality, lm_logboth)

lm_table = sjt.lm(lmlist, 
                  show.header = FALSE, 
                  depvar.labels = c("Model 1: Mortality", "Model 2: Mortality", "Model 3: log(Mortality)", "Model 4:     log(mortality)"),
                  no.output = TRUE)

```

```{r, echo = FALSE}

lm_table

```

The above table confirms that the model with log wine consumption and log heart disease mortality rate has the best fit. It has the highest R squared and adjusted R squared, with values .738 / .722 respectively. Therefore, I would say model 4 is the best. 

####c. Interpret each regression model by describing the change in mortality for a given change in the predictor. Use an increase of 10 units of wine consumption for linear wine and a doubling of wine consumption for logarithmic wine.

Using the model names from the table in part (b):

Model 1: An increase of 10 units of wine consumption is associated with a .8 decrease in the heart disease mortality rates.

Model 2: A doubling of wine consumption is associated with a 1.22 decrease in heart disease mortality rate. (-1.77*log(2) = -1.22)

Model 3: An increase of 10 units of wine consumption is associated with an 18 percent decrease in heart disease mortality rate. (exp(-.02) = .82, meaning that heart disease mortality is at .82 of its original value.)

Model 4: A doubling of wine consumption is associated with a 22 percent decrease in heart disease mortality rate. (2^-.36 = .78, meaning that heart disease is at 78 percent of its original value)

#Question 2: Election

####a.The data in File Bush.xls contain the numbers of votes for Buchanan and Bush in all 67 counties in Florida. What evidence is there in the scatterplot of Display 8.25 that Buchanan received more votes than expected in Palm Beach County? Analyze the data without Palm Beach County results to obtain an equation for predicting Buchanan votes from Bush votes. Obtain a 95% prediction interval for the number of Buchanan votes in Palm Beach from this result-assuming the relationship is the same in this county as in the others. If it is assumed that Buchanan's actual count contains a number of votes intended for Gore, what can be said about the likely size of this number from the prediction interval? (Consider transformation.)

```{r, echo = FALSE, results="hide", message=FALSE}

Bush <- read_csv("Bush.csv")

```


#####a1. What evidence is there in the scatterplot of Display 8.25 that Buchanan received more votes than expected in Palm Beach County?

From the 8.25 scatter plot, it appears that Palm Beach County has an unusually high number of Buchanan votes as compared to the other counties with a similar number of bush votes. 

#####a2. Analyze the data without Palm Beach County results to obtain an equation for predicting Buchanan votes from Bush votes. Obtain a 95% prediction interval for the number of Buchanan votes in Palm Beach from this result-assuming the relationship is the same in this county as in the others.

```{r}

### Remove palm beach from the Bush data set
nopalm = Bush %>%
  filter(county != "PALM BEACH")

### Run a linear regression on the filtered data
palmmodel = lm(buchanan2000 ~ bush2000, data = nopalm)

### Predict the number of votes for Buchanan when the number of Bush votes = 152846
predict(palmmodel, data.frame(bush2000 = 152846), interval = "predict")

```

Our predicted number of votes for Buchanan based on the number of votes for Bush in Palm county is 597.8 with a 95% confidence interval of [364.7 , 830.8]. 

#####a3. If it is assumed that Buchanan's actual count contains a number of votes intended for Gore, what can be said about the likely size of this number from the prediction interval? (Consider transformation.)

Although our predicted number of votes for Buchanan based on the number of votes for George Bush was 598, the actual number was 3407. So there is evidence to suggest that the interval [3407-830.8 , 3407-364.7] = [ 2576.2, 3042.3]  contains the amount of Buchanan votes that were actually intended for Gore. 

####b. One might suspect that the prediction interval can be narrowed and the validity of the procedure strength¬ened by incorporating other relevant predictor variables. The dataset gives the vote counts by county in Florida for Buchanan and for four other presidential candidates in 2000, along with the total vote counts in 2000, the presidential vote counts for three presidential candidates in 1996, the vote count for Buchanan in his only other campaign in Florida¬, the 1996 Republican primary, the registration in Buchanan's Reform party, and the total political party registration in the county.

#### Analyze the data in Ex1222 and write a statistical summary predicting the number of Buchanan, votes in Palm Beach Country that were not intended for him. It would be appropriate to describe any unverifiable assumptions used in applying the prediction equation for this purpose. (Suggestion: Find a model for predicting Buchanan's 2000 vote from other variables, excluding Palm Beach County, which is listed last in the data set. Consider a transformation of all counts.)


Here is the model with all of the other variables included:

```{r}

### Run linear model with other variables in Bush dataset
bmodel = lm(buchanan2000 ~
              gore2000 +
              nader2000 +
              browne2000 +
              total2000 +
              clinton96 +
              dole96 +
              perot96 +
              buchanan96p +
              reform.reg +
              total.reg, data = nopalm)

### Summary of model
summary(bmodel)

### Predict the number of votes for Palm County
predict(bmodel, data.frame(Bush[67,]), interval = "predict")

```

Let's examine the independent variables of the above model to see if they appear linear:

```{r}

pairs(~buchanan2000 +
              gore2000 +
              nader2000 +
              browne2000 +
              total2000 +
              clinton96 +
              dole96 +
              perot96 +
              buchanan96p +
              reform.reg +
              total.reg, 
              data = nopalm)

```

We can see from the top row of the pairs plot that the is a positive correlation between the number of votes for Buchanan and the rest of the variables. However, most of the plots show heterskedasticity: the variance in the y values at the high x values seems higher than the variance at the low x values. Let's see if we can fix this using log transformations of all the variables:

```{r, warning=FALSE}
###replace 0 with .0001
nopalm$reform.reg = nopalm$reform.reg+.0001
```

```{r}

pairs(~buchanan2000 +
              gore2000 +
              nader2000 +
              browne2000 +
              total2000 +
              clinton96 +
              dole96 +
              perot96 +
              buchanan96p +
              reform.reg +
              total.reg,
              log = "xy",
              data = nopalm)

```

The linearity of the logged variables is considerably stronger than the untransformed case. 

Let's run a linear model using the logged variables:

```{r}


bmodel2 = lm(log(buchanan2000) ~
              log(gore2000) +
              log(nader2000) +
              log(browne2000) +
              log(total2000) +
              log(clinton96) +
              log(dole96) +
              log(perot96) +
              log(buchanan96p) +
              log(reform.reg) +
              log(total.reg), 
              data = nopalm)

summary(bmodel2)

```

Using this log model let's predict the number of votes for Buchanan for Palm Beach county:

```{r}

### Log all the values in the Bush dataframe
loggedbush = Bush
loggedbush$reform.reg = loggedbush$reform.reg + .0001
cols = names(loggedbush)
cols = cols[2:length(cols)]
loggedbush[cols] <- log(loggedbush[cols])

predict(bmodel2, data.frame(loggedbush[67,]), interval = "predict")


```

This predicted interval for the log tranformed model means that we would expect the number of Buchanan votes to be 2.2% less than the mean number of Buchanan votes. 

Thus the expected number of votes for Palm County in this model would be:

```{r}

mean(Bush$buchanan2000) * (1-.022)

```

Since our observed count was 3407, there is some plausibility in thinking that there were some votes cast for Buchanan that were meant for Gore, maybe even around 3154 (which would have swayed the election).


#Question 3: Blood Brain Experiment

```{r, echo = FALSE, message= FALSE}

BloodBrain <- read_csv("BloodBrain.csv")

```

####a.	Compute "Jittered" versions of-treatment, days after inoculation, and an indicator variable for females by adding small random numbers to each (uniform random numbers between -.15 and .15 work well). Or you could use the jitter function.

```{r}

###Add jitter to columns
jitterbrain = BloodBrain
jitterbrain$DAYS = jitter(jitterbrain$DAYS)
jitterbrain$TREAT = jitter(as.numeric(as.factor(jitterbrain$TREAT)))
jitterbrain$SEX = jitter(as.numeric(as.factor(jitterbrain$SEX)))

```

####b. Obtain a matrix of scatter plots for the following variables: log sacrifice time, treatment (jittered), days after inoculation (jittered), sex (jittered), and the log of the brain tumor-to-liver antibody ratio. Use the function pairs in the graphics package or scatterplotMatrix in the car package.

```{r}

pairs(data = jitterbrain, ~ log(BRAIN/LIVER) + log(TIME) + TREAT + DAYS + SEX )

```


####c. Obtain a matrix of the correlation coefficients among the same five variables (not jittered).

```{r}

### Set up dataframe for the correlation matrix
braincordata = BloodBrain %>%
  select(TIME, TREAT, DAYS, SEX, BRAIN, LIVER)
braincordata = braincordata %>%
  mutate(RATIO = BRAIN/LIVER)
braincordata = braincordata %>%
  select(-BRAIN, - LIVER)
braincordata$TIME = log(braincordata$TIME)
braincordata$RATIO = log(braincordata$RATIO)
braincordata$TREAT = as.numeric(as.factor(braincordata$TREAT))
braincordata$SEX = as.numeric(as.factor(braincordata$SEX))

### Command for computing the correlation matrix
cor(braincordata)

```

In the above correlation matrix:
  - TIME = log(TIME)
  - RATIO = log(RATIO)

####d.	On the basis of this, what can be said about the relationship between the covariates (sex and days after inoculation), the response, and the design variables (treatment and sacrifice time).

We can see from the correlation matrix above that sex and the log ratio has a moderately weak correlation. Days after inoculation and the log Ratio have a weak correlation. The treatment group has a very weak and slightly negative correlation with the log Ratio. Lastly, the sacrifice time has a very strong positive correlation with the response log ratio.

####e.	Fit the regression of the log response (brain tumor-to-liver antibody ratio) on an indicator variable for treatment and on sacrifice time treated as a factor with four levels (include three indicator variables, for sacrifice time == 3, 24, and 72 hours). Use the model to find the estimated mean of the log response at each of the eight treatment combinations (all combinations of the two infusions and the four sacrifice times).

```{r}

### Run the linear model
lm_part_e = lm(log(BRAIN / LIVER) ~ as.factor(TIME) + as.factor(TREAT), data = BloodBrain)

### Create data frame with all combos
the_treats = unique(as.factor(BloodBrain$TREAT))
the_times = unique(as.factor(BloodBrain$TIME))
combos = data.frame(TREAT = rep(the_treats, each =  4), TIME = rep(the_times, 2))

### Estimate the mean for each of the 8 combinations
predictions = predict(lm_part_e, combos)

### Add these predictions to our combination table
combos$logMEAN = predictions

### View Table
kable(combos)

```


####f. Let X represent log of sacrifice time. Fit the regression of the log response on an indicator variable for treatment, X, X^2, and X^3. Use the estimated model to find the estimated mean of the log response at each of the eight treatment combinations.

```{r}

BloodBrain$LTIME2 = log(BloodBrain$TIME)^2
BloodBrain$LTIME3 = log(BloodBrain$TIME)^3

### Run the linear model
lm_part_f = lm( log(BRAIN / LIVER) ~ as.factor(TREAT) + log(TIME) + LTIME2 + LTIME3, data = BloodBrain)

summary(lm_part_f)


### Create data frame with all combos
the_treatsf = unique(as.factor(BloodBrain$TREAT))
the_timesf = unique(log(BloodBrain$TIME))
the_timesf2 = unique(BloodBrain$LTIME2)
the_timesf3 = unique(BloodBrain$LTIME3)
combosf = data.frame(TREAT = rep(the_treatsf, each =  4), TIME = rep(the_timesf, 2), LTIME2 = rep(the_timesf2, 2),
                     LTIME3 = rep(the_timesf3, 2))

predictionsf = predict(lm_part_f, combosf)

combosf$logMEAN = predictions

kable(combosf)

```

####g. Why are the answers to parts (5) and (6) the same?

The results are the same because time is effectively turned into a factor when it is included in the regression in part 6 three times. Since the times are split in a way that effectively makes them categorical, their beta coefficient is the mean between the different categories. 

####h. Fit the regression of the log response (brain tumor-to-liver antibody ratio) on all covariates, the treatment indicator, and sacrifice time, treated as a factor with four levels (include three indicator variables, for sacrifice time == 3, 24, and 72 hours).

```{r}

###Run the linear model
lm_part_h = lm(log(BRAIN / LIVER) ~ as.factor(TREAT) + as.factor(TIME) + DAYS + SEX + WEIGHT + LOSS + TUMOR, data = BloodBrain)

###Summarize the model
summary(lm_part_h)

```

####i. Obtain a set of case influence statistics, including a measure of influence, the leverage, and the studentized residua1.

```{r}

inf_measures = as.data.frame(influence.measures(lm_part_h)[[1]])

```


```{r, fig.width=12}

ggplot(stack(inf_measures), aes(x = ind, y = values)) +
  geom_boxplot() + ggtitle("Influence Measures") +
  theme_minimal()

```

```{r, echo = FALSE}

plot(lm_part_h)

```

####j. Discuss whether any influential observations or outliers occur with respect to this fit.

We can see from the "Influence Measures" plot that there are points in each of the beta boxplots that change the beta value by more than half a standard deviation (especially at outlier points). Some points change the beta by over 2 standard deviations. Thus, there are certainly influential points in this fit. We also see that there are five points which change the fit of the model by over 1 standard deviation when removed from the model. We see from the cooks distance boxplot in "Influence Measures" that there are three points that are outliers. These are within 1 standard deviation of the mean distance, so they are weak leverage points. Overall, there are more influence points than there are leverage points in this model. 

From the "Residuals vs Fitted" plot we can see that the residuals seem randomly scattered about the zero line. Thus there is no evidence of heteroskedisticity. 

#Question 4: 

